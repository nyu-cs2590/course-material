{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Operations on word vectors\n",
    "\n",
    "In this notebook, we will cover three different operations on word vectors:\n",
    "\n",
    "1. Similarity: Do similar words have similar word vectors?\n",
    "2. Word Analogy Tasks: Can the similarity of word vectors be used to solve analogy tasks like \"a is to b as c is to what\"?\n",
    "3. Bias in word vectors: Do word vectors encode biases from the data, and can we remove these biases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code heavily based on https://github.com/gemaatienza/Deep-Learning-Coursera/blob/master/5.%20Sequence%20Models/Operations%20on%20word%20vectors%20-%20v2.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import collections\n",
    "import os\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        \n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            \n",
    "    return words, word_to_vec_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this notebook, we will not be training word vectors. We will instead use pre-trained word vectors. More specically, we will use the [GloVe](https://aclanthology.org/D14-1162.pdf) word vectors trained on 6 billion tokens, where every vector is of 50 dimension. \n",
    "\n",
    "You can download the word vectors from [here](https://nlp.stanford.edu/projects/glove/) or [here](https://www.kaggle.com/datasets/watts2/glove6b50dtxt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n",
    "\n",
    "# words -- set of words in the vocabulary.\n",
    "# word_to_vec_map -- dictionary mapping words to their respective vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cosine similarity function\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    # Compute the dot product between u and v\n",
    "    dot = np.dot(u,v)\n",
    "    # Compute the L2 norm of u\n",
    "    norm_u = np.linalg.norm(u)\n",
    "    \n",
    "    # Compute the L2 norm of v\n",
    "    norm_v = np.linalg.norm(v)\n",
    "    # Compute the cosine similarity defined by formula\n",
    "    cosine_similarity = dot/(norm_u*norm_v)\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us evaluate if relation words (e.g. mother, brother etc) are more similar to each other than to a random word (e.g. ball)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(mother, father) =  0.8909038442893615\n",
      "cosine_similarity(mother, brother) =  0.932262758630331\n",
      "cosine_similarity(mother, ball) =  0.37516525380179355\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Are relation words more similar to each other?\n",
    "\n",
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "brother = word_to_vec_map[\"brother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "\n",
    "print(\"cosine_similarity(mother, father) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(mother, brother) = \", cosine_similarity(father, brother))\n",
    "print(\"cosine_similarity(mother, ball) = \", cosine_similarity(father, ball))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us check if countries are more similar to each other than a random word. Do you observe in trend in terms on which two countries have more similar word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_similarity(france, italy) =  0.7788637392080094\n",
      "cosine_similarity(france, canada) =  0.6477412715377313\n",
      "cosine_similarity(france, japan) =  0.4044382941502501\n",
      "cosine_similarity(france, ball) =  0.25069385386555776\n"
     ]
    }
   ],
   "source": [
    "# Are countries more similar to each other?\n",
    "\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "japan = word_to_vec_map[\"japan\"]\n",
    "canada = word_to_vec_map[\"canada\"]\n",
    "\n",
    "print(\"cosine_similarity(france, italy) = \", cosine_similarity(france, italy))\n",
    "print(\"cosine_similarity(france, canada) = \", cosine_similarity(france, canada))\n",
    "print(\"cosine_similarity(france, japan) = \", cosine_similarity(france, japan))\n",
    "print(\"cosine_similarity(france, ball) = \", cosine_similarity(france, ball))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Analogy Task -- we will first write the core function which returns the best answer. We will do this iterating over all words in the vocabulary, and computing the similarity of the difference (a - b, c - d)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Word Analogy\n",
    "\n",
    "# Function to find the best word which fits the analogy\n",
    "\n",
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
    "    e_a = word_to_vec_map.get(word_a)\n",
    "    e_b = word_to_vec_map.get(word_b)\n",
    "    e_c = word_to_vec_map.get(word_c)\n",
    "    # e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity(np.subtract(e_b,e_a), np.subtract(word_to_vec_map.get(w),e_c))\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us try out some examples. In some cases the relation between a and b is more semantic (e.g. capital of a country) than in other cases where (e.g. comparative vs superlative). Note that these word vectors might not work for all different kinds of relations (such as the last one showed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "italy -> italian :: spain -> spanish\n",
      "india -> delhi :: japan -> tokyo\n",
      "man -> woman :: boy -> girl\n",
      "small -> smaller :: large -> larger\n",
      "small -> smaller :: big -> competitors\n"
     ]
    }
   ],
   "source": [
    "# May take 1-2 minutes to run\n",
    "\n",
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'),\n",
    "                 ('man', 'woman', 'boy'), ('small', 'smaller', 'large'),\n",
    "                 ('small', 'smaller', 'big')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias in Word Vectors --- we will start off by identifying the direction corresponding to a particular concept (e.g. gender), and then compute similarity of common occupations with this concept. Note that a positive value indicates similarity with 'woman' more than 'man', and a negative value indicates the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of names and their similarities with constructed vector:\n",
      "homemaker 0.4272061180602086\n",
      "nanny 0.28719293921749317\n",
      "nurse 0.38030879680687524\n",
      "receptionist 0.3307794175059374\n",
      "magician -0.2385883242639807\n",
      "architect -0.18345345423169837\n",
      "boss -0.4439767394197472\n",
      "scientist -0.05193035281313462\n"
     ]
    }
   ],
   "source": [
    "### Debiasing word vectors\n",
    "\n",
    "# Compute the vector for gender\n",
    "g = word_to_vec_map['woman'] - word_to_vec_map['man']\n",
    "\n",
    "# Conider the cosine similarity of different words with g\n",
    "\n",
    "print ('List of names and their similarities with constructed vector:')\n",
    "\n",
    "# Word list\n",
    "name_list = ['homemaker', 'nanny', 'nurse', 'receptionist', 'magician', 'architect', 'boss', 'scientist']\n",
    "\n",
    "for w in name_list:\n",
    "    print (w, cosine_similarity(word_to_vec_map[w], g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To debias word projects, we will define a function which projects the vectors onto the concept and then subtracts it from the original vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def neutralize(word, g, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Removes the bias of \"word\" by projecting it on the space orthogonal to the bias axis. \n",
    "    This function ensures that gender neutral words are zero in the gender subspace.\n",
    "    \n",
    "    Arguments:\n",
    "        word -- string indicating the word to debias\n",
    "        g -- numpy-array of shape (50,), corresponding to the bias axis (such as gender)\n",
    "        word_to_vec_map -- dictionary mapping words to their corresponding vectors.\n",
    "    \n",
    "    Returns:\n",
    "        e_debiased -- neutralized word vector representation of the input \"word\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select word vector representation of \"word\". Use word_to_vec_map. (≈ 1 line)\n",
    "    e = word_to_vec_map[word]\n",
    "    \n",
    "    # Compute e_biascomponent using the formula give above. (≈ 1 line)\n",
    "    e_biascomponent = (np.dot(e,g)/np.linalg.norm(g)**2)*g\n",
    " \n",
    "    # Neutralize e by substracting e_biascomponent from it \n",
    "    # e_debiased should be equal to its orthogonal projection. (≈ 1 line)\n",
    "    e_debiased = e-e_biascomponent\n",
    "    \n",
    "    return e_debiased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate our debiasing method, we will compare the similarity of the original and debiased vectors with the vectors corresponding to our concept (e.g. gender)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity between receptionist and g, before neutralizing:  0.3307794175059374\n",
      "cosine similarity between receptionist and g, after neutralizing:  -2.099120994400013e-17\n",
      "cosine similarity between boss and g, before neutralizing:  -0.4439767394197472\n",
      "cosine similarity between boss and g, after neutralizing:  -8.569698399890252e-17\n"
     ]
    }
   ],
   "source": [
    "e = \"receptionist\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"receptionist\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"receptionist\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))\n",
    "\n",
    "e = \"boss\"\n",
    "print(\"cosine similarity between \" + e + \" and g, before neutralizing: \", cosine_similarity(word_to_vec_map[\"boss\"], g))\n",
    "\n",
    "e_debiased = neutralize(\"boss\", g, word_to_vec_map)\n",
    "print(\"cosine similarity between \" + e + \" and g, after neutralizing: \", cosine_similarity(e_debiased, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "8hb5s",
   "launcher_item_id": "5NrJ6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f92b2924b84ff19c1c3dc485f7644d4486f64738191026bf8e6de303969141b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
