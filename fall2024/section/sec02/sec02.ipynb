{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnY-CRUecFCD"
      },
      "source": [
        "# Section 2: Operations on word vectors\n",
        "\n",
        "In this notebook, we will cover three different operations on word vectors:\n",
        "\n",
        "1. Similarity: Do similar words have similar word vectors?\n",
        "2. Word Analogy Tasks: Can the similarity of word vectors be used to solve analogy tasks like \"a is to b as c is to what\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEhR238pcFCE"
      },
      "outputs": [],
      "source": [
        "## Code heavily based on https://github.com/gemaatienza/Deep-Learning-Coursera/blob/master/5.%20Sequence%20Models/Operations%20on%20word%20vectors%20-%20v2.ipynb\n",
        "\n",
        "import numpy as np\n",
        "import collections\n",
        "import os\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "\n",
        "    return words, word_to_vec_map\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ulii9zoEcFCF"
      },
      "source": [
        "For the purpose of this notebook, we will not be training word vectors. We will instead use pre-trained word vectors. More specically, we will use the [GloVe](https://aclanthology.org/D14-1162.pdf) word vectors trained on 6 billion tokens, where every vector is of 50 dimension.\n",
        "\n",
        "You can download the word vectors from [here](https://nlp.stanford.edu/projects/glove/) or [here](https://www.kaggle.com/datasets/watts2/glove6b50dtxt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCmOB2RVcFCF"
      },
      "outputs": [],
      "source": [
        "\n",
        "words, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n",
        "\n",
        "# words -- set of words in the vocabulary.\n",
        "# word_to_vec_map -- dictionary mapping words to their respective vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuwPyPJgcFCF"
      },
      "outputs": [],
      "source": [
        "# Define cosine similarity function\n",
        "\n",
        "def cosine_similarity(u, v):\n",
        "    \"\"\"\n",
        "    Cosine similarity reflects the degree of similariy between u and v\n",
        "\n",
        "    Arguments:\n",
        "        u -- a word vector of shape (n,)\n",
        "        v -- a word vector of shape (n,)\n",
        "\n",
        "    Returns:\n",
        "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
        "    \"\"\"\n",
        "\n",
        "    distance = 0.0\n",
        "\n",
        "    # Compute the dot product between u and v\n",
        "    dot = np.dot(u,v)\n",
        "    # Compute the L2 norm of u\n",
        "    norm_u = np.linalg.norm(u)\n",
        "\n",
        "    # Compute the L2 norm of v\n",
        "    norm_v = np.linalg.norm(v)\n",
        "    # Compute the cosine similarity defined by formula\n",
        "    cosine_similarity = dot/(norm_u*norm_v)\n",
        "\n",
        "    return cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wcJVr9VcFCG"
      },
      "source": [
        "First, let us evaluate if relation words (e.g. mother, brother etc) are more similar to each other than to a random word (e.g. ball)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hhC3oYHcFCG",
        "outputId": "41f55e61-e52e-4635-ad99-50cfa653e210"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cosine_similarity(mother, father) =  0.8909038442893615\n",
            "cosine_similarity(mother, brother) =  0.932262758630331\n",
            "cosine_similarity(mother, ball) =  0.37516525380179355\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Are relation words more similar to each other?\n",
        "\n",
        "father = word_to_vec_map[\"father\"]\n",
        "mother = word_to_vec_map[\"mother\"]\n",
        "brother = word_to_vec_map[\"brother\"]\n",
        "ball = word_to_vec_map[\"ball\"]\n",
        "\n",
        "print(\"cosine_similarity(mother, father) = \", cosine_similarity(father, mother))\n",
        "print(\"cosine_similarity(mother, brother) = \", cosine_similarity(father, brother))\n",
        "print(\"cosine_similarity(mother, ball) = \", cosine_similarity(father, ball))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNWwwLmXcFCG"
      },
      "source": [
        "Next, let us check if countries are more similar to each other than a random word. Do you observe in trend in terms on which two countries have more similar word vectors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGtrLL1ecFCG",
        "outputId": "59b706db-3bb4-40e9-cc1d-0cab514741b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cosine_similarity(france, italy) =  0.7788637392080094\n",
            "cosine_similarity(france, canada) =  0.6477412715377313\n",
            "cosine_similarity(france, japan) =  0.4044382941502501\n",
            "cosine_similarity(france, ball) =  0.25069385386555776\n"
          ]
        }
      ],
      "source": [
        "# Are countries more similar to each other?\n",
        "\n",
        "france = word_to_vec_map[\"france\"]\n",
        "italy = word_to_vec_map[\"italy\"]\n",
        "japan = word_to_vec_map[\"japan\"]\n",
        "canada = word_to_vec_map[\"canada\"]\n",
        "\n",
        "print(\"cosine_similarity(france, italy) = \", cosine_similarity(france, italy))\n",
        "print(\"cosine_similarity(france, canada) = \", cosine_similarity(france, canada))\n",
        "print(\"cosine_similarity(france, japan) = \", cosine_similarity(france, japan))\n",
        "print(\"cosine_similarity(france, ball) = \", cosine_similarity(france, ball))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJjmuFKRcFCG"
      },
      "source": [
        "Word Analogy Task -- we will first write the core function which returns the best answer. We will do this iterating over all words in the vocabulary, and computing the similarity of the difference (a - b, c - d)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnrnepz1cFCH"
      },
      "outputs": [],
      "source": [
        "## Word Analogy\n",
        "\n",
        "# Function to find the best word which fits the analogy\n",
        "\n",
        "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
        "    \"\"\"\n",
        "    Performs the word analogy task as explained above: a is to b as c is to ____.\n",
        "\n",
        "    Arguments:\n",
        "    word_a -- a word, string\n",
        "    word_b -- a word, string\n",
        "    word_c -- a word, string\n",
        "    word_to_vec_map -- dictionary that maps words to their corresponding vectors.\n",
        "\n",
        "    Returns:\n",
        "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
        "    \"\"\"\n",
        "\n",
        "    # convert words to lower case\n",
        "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
        "\n",
        "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
        "    e_a = word_to_vec_map.get(word_a)\n",
        "    e_b = word_to_vec_map.get(word_b)\n",
        "    e_c = word_to_vec_map.get(word_c)\n",
        "    # e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
        "\n",
        "    words = word_to_vec_map.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "\n",
        "    # loop over the whole word vector set\n",
        "    for w in words:\n",
        "        # to avoid best_word being one of the input words, pass on them.\n",
        "        if w in [word_a, word_b, word_c] :\n",
        "            continue\n",
        "\n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
        "        cosine_sim = cosine_similarity(np.subtract(e_b,e_a), np.subtract(word_to_vec_map.get(w),e_c))\n",
        "\n",
        "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
        "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "\n",
        "    return best_word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on9ChOaecFCH"
      },
      "source": [
        "Next, let us try out some examples. In some cases the relation between a and b is more semantic (e.g. capital of a country) than in other cases where (e.g. comparative vs superlative). Note that these word vectors might not work for all different kinds of relations (such as the last one showed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnr8rAKkcFCH",
        "outputId": "4c2548f1-b7e7-4dd0-e81c-fdba710ce984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "italy -> italian :: spain -> spanish\n",
            "india -> delhi :: japan -> tokyo\n",
            "man -> woman :: boy -> girl\n",
            "small -> smaller :: large -> larger\n",
            "small -> smaller :: big -> competitors\n"
          ]
        }
      ],
      "source": [
        "# May take 1-2 minutes to run\n",
        "\n",
        "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'),\n",
        "                 ('man', 'woman', 'boy'), ('small', 'smaller', 'large'),\n",
        "                 ('small', 'smaller', 'big')]\n",
        "for triad in triads_to_try:\n",
        "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "course_slug": "nlp-sequence-models",
      "graded_item_id": "8hb5s",
      "launcher_item_id": "5NrJ6"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "f92b2924b84ff19c1c3dc485f7644d4486f64738191026bf8e6de303969141b5"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}