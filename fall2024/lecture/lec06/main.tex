\documentclass[usenames,dvipsnames,notes,11pt,aspectratio=169,hyperref={colorlinks=true, linkcolor=blue}]{beamer}
\usepackage{ifthen}
\usepackage{xcolor}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{centernot}
\usepackage{pifont}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{cuted}
\usepackage{booktabs}
\usepackage{array}
\usepackage{textcomp}
\usepackage{setspace}
\usepackage{xspace}
\usepackage{subcaption}
\usepackage{tikz}
\usepackage{pdfcomment}
%\newcommand{\pdfnote}[1]{\marginnote{\pdfcomment[icon=note]{#1}}}
\newcommand{\pdfnote}[1]{}

\usepackage{pgfpages}
%\setbeameroption{show notes on second screen}


\input ../beamer-style
\input ../std-macros
\input ../macros

\newcommand{\pt}{\partial}

\AtBeginSection[]
{
    \begin{frame}
        \frametitle{Table of Contents}
        \tableofcontents[currentsection]
    \end{frame}
}
\parskip=10pt

\title[DS-GA.1011]{Pretraining and Finetuning}
\author[He He]{He He
}
\institute[NYU]{
    \includegraphics[height=1cm]{../figures/nyu-logo}\\
}
\date{October 9, 2023}

\begin{document}
\begin{frame}
\titlepage
\end{frame}

\section{Representation learning}

\begin{frame}
    {Representation learning}
    What are good representations?\\
    \begin{itemize}
        \item Enable a notion of distance over text (word embeddings)
        \item Contains good features for downstream tasks 
    \end{itemize}

    \pause
    Examples:
        \begin{tabular}{ll}
            negative & the food is good but doesn't worth an hour wait
        \end{tabular}

    \begin{itemize}
        \item Simple features (e.g. BoW) require complex models.\\
        \item \blue{Good features} only need \blue{simple models} (e.g. linear classifier) .
    \vspace{-1em}
    \begin{figure}
        \includegraphics[height=3cm]{figures/sentiment}
        \caption{\href{https://arxiv.org/abs/1704.01444}{Sentiment neuron} [Radford et al., 2017]}
    \end{figure}
    \end{itemize}
\end{frame}

\begin{frame}
    {Representation learning}
    What can we do with good representations:\\
    \begin{itemize}
        \item Learning with small data: fine-tuning learned representations
        \item Transfer learning: one model/representation for many tasks 
        \item Metric learning: get a similarity metric for free 
    \end{itemize}

    \pause\bigskip
    How to obtain such a representation:
    \begin{itemize}
        \item Training a neural network on any task gives us a representation good for \textit{that task}.
        \item But on which task can we learn good \textit{general} representations?
    \end{itemize}
\end{frame}

\begin{frame}
    {What can we learn from word guessing?}
    \begin{itemize}
        \itemsep1em
        \item The cats that are raised by my sister \rule{1.5cm}{0.5mm} sleeping. \pause\hfill \textit{syntax}
        \item Jane is happy that John invited \rule{1.5cm}{0.5mm} friends to his birthday party. \pause\hfill \textit{coreference}
        \item \rule{1.5cm}{0.5mm} is the capital of Tanzania. \pause\hfill \textit{knowledge}
        \item The boy is \rule{1.5cm}{0.5mm} because he lost his keys.  \pause\hfill \textit{commonsense}
        \item John took 100 bucks to Vegas. He won 50 and then lost 100. Now he only has \rule{1.5cm}{0.5mm} to go home. \pause\hfill \textit{numerical reasoning}
    \end{itemize}

    \pause\medskip
    Word guessing entails many tasks related to language understanding!

    %\think{But aren't we already doing this in skip-gram / CBOW?}
\end{frame}

\begin{frame}
    {Self-supervised learning}

    \textbf{Key idea}: predict parts of the input from the rest\\
    \begin{itemize}
        \item \blue{No supervision} is needed---both input and output are from the raw data.
        \item Easy to \blue{scale}---only need unlabeled data.
        \item Learned representation is \blue{general}---useful for many tasks.
    \end{itemize}

    \pause
    \textbf{Approach}:\\
    \begin{itemize}
        \item \textbf{Pretrain}: train a model using self-supervised learning objectives on large data.
        \item \textbf{Finetune}: update part or all of the parameters of the pretrained model (which provides an initialization) on labeled data of a downstream task.
    \end{itemize}
\end{frame}

\begin{frame}
    {A bit of history}
    \begin{itemize}[<+->]
        \item Pretrain an \blue{RNN} model on unlabeled data and finetune on supervised tasks
            \href{https://arxiv.org/pdf/1511.01432.pdf}{[Dai et al., 2015]}
            \href{https://arxiv.org/pdf/1511.01432.pdf}{[ULMFiT; Howard et al., 2018]}
            \begin{itemize}[<.->]
                \item Promising results on a small scale
            \end{itemize}
        \item ELMo: replace static word embedding by \blue{contextual word embeddings} from pretrained \blue{bi-LSTM} \href{https://arxiv.org/abs/1802.05365}{[Peters et al., 2018]}
            \begin{itemize}[<.->]
                \item First impactful result in NLP
            \end{itemize}
        \item Pretrain a \blue{Transformer} model and finetune on supervised tasks 
            \begin{itemize}[<.->]
                \item GPT \href{https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}{[Radford et al., 2018]},
                    BERT \href{https://arxiv.org/abs/1810.04805}{[Devlin et al., 2018]}
            \end{itemize}
        \item \blue{Scale} the pretrained model to larger sizes
            \begin{itemize}[<.->]
                \item GPT-2 (1.5B), T5 (11B), GPT-3 (175B), PaLM (540B) 
                \item We will talk about 100B+ models in the third module
            \end{itemize}
    \end{itemize}
\end{frame}

\section{Architectures of pretrained models}

\begin{frame}
    {Types of pretrained models}
   
    \begin{itemize}[<+->]
        \itemsep1em
        \item \textbf{Encoder models}, e.g., BERT
            \begin{itemize}[<.->]
                \item Encode text into vector representations that can be used for downstream classification tasks
            \end{itemize}
        \item \textbf{Encoder-decoder models}, e.g., T5
            \begin{itemize}[<.->]
                \item Encode input text into vector representations and generate text conditioned on the input
            \end{itemize}
        \item \textbf{Decoder models}, e.g., GPT-2
            \begin{itemize}[<.->]
                \item Read in text (prefix) and continue to generate text
            \end{itemize}
    \end{itemize}

    \pause\medskip
    Current pretrained models are all transformer based.
\end{frame}

\begin{frame}
    {Encoder models}
    
    An encoder takes a sequence of tokens and output their {\em contextualized} representations:
    $$
        h_1,\ldots,h_n = \mathrm{Encoder}(x_1,\ldots,x_n)
    $$
    We can then use $h_1,\ldots,h_n$ for other tasks.

    \pause\bigskip
    How do we train an $\mathrm{Encoder}$?\\
    \begin{itemize}
        \item Use any \blue{supervised} task: $y=f(h_1,\ldots,h_n)$
        \item Use \blue{self-supervised} learning: predict a word from its context 
    \end{itemize}
\end{frame}

\begin{frame}
    {Masked language modeling}

    \begin{tabular}{ccccc}
        ? & language & processing & is & ?
    \end{tabular}

    \pause\medskip
    Learning objective (MLE):\\
    $$
    \max \sum_{x\in\sD, i\sim p_{\text{mask}}} \log p(x_i\mid x_{-i}; \theta)
    $$
    \vspace{-1em}
    \begin{itemize}
        \item $x$: a sequence of tokens sampled from a corpus $\sD$\\
            {\em natural language processing is fun}
        \item $p_{\text{mask}}$: mask generator\\
            Sample two positions uniformly at random, e.g., 1 and 5
        \item $x_{-i}$: noisy version fo $x$ where $x_i$ is corrupted\\
            {\em [MASK] language processing is [MASK]}
    \end{itemize}
\end{frame}

\begin{frame}
    {BERT: objective}

        \begin{itemize}
            \item \textbf{Masked language modeling}:
                \begin{itemize}
                    \item Randomly sample 15\% tokens as prediction targets
                    \item Replace the target tokens by \texttt{[MASK]} or a random token, or leave it unchanged
                        \begin{itemize}
                            \item[] cats \blue{are} cute $\rightarrow$
                        cats \blue{\texttt{[MASK]}/is/are} cute
                        \end{itemize}
                    \item Later work has shown that just use \texttt{[MASK]} is sufficient
                \end{itemize}
            \pause
            \item \textbf{Next sentence prediction}: predict whether a pair of sentences are consecutive
                $$
                \max \sum_{x\sim\sD, x_{n}\sim p_{\text{next}}} \log p(y\mid x, x_n; \theta)
                $$
                \vspace{-1em}
                \begin{itemize}
                    \item $x_n$: either the sentence following $x$ or a randomly sampled sentence
                    \item $y$: binary label of whether $x_n$ follows $x$
                    \item Later work has shown that this objective is not necessary 
                \end{itemize}
        \end{itemize}
\end{frame}

\begin{frame}
    {BERT: architecture}
    \begin{figure}
            \includegraphics[width=.9\textwidth]{figures/bert}
    \end{figure}
    \vspace{-1em}
    \begin{itemize}[<+->]
        \item Tokenization: wordpiece (similar to byte pair encoding) (see \href{https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt}{details})
        \item \texttt{[CLS]}: first token of all sequences; used for next sentence prediction
        \item Distinguish two sentences in a pair: \texttt{[SEP]} and segment embedding
        \item Learned position embedding
        \item 12 (base; 110M params) or 24 (large; 340M params) layer Transformer
    \end{itemize}
\end{frame}

\begin{frame}
    {Finetuning BERT}
        Classification tasks:
            Add a linear layer (randomly initialized) on top of the \texttt{[CLS]} embedding
            $$
            p(y\mid x) = \mathrm{softmax}(Wh_{\text{[CLS]}}+b)
            $$
            \begin{figure}
                \includegraphics[width=0.8\textwidth]{figures/bert-classification}
            \end{figure}
\end{frame}

\begin{frame}
    {Finetuning BERT}
        Sequence labeling tasks:
            Add linear layers (randomly initialized) on top of every token 
            $$
            p(y_i \mid x) = \mathrm{softmax}(Wh_{i}+b)
            $$
            \begin{figure}
                \includegraphics[width=0.8\textwidth]{figures/bert-seq-label}
            \end{figure}
\end{frame}

\begin{frame}
    {Finetuning BERT}

    \begin{itemize}
        \item Finetune all parameters (both the newly added layer and the pretrained weights)
        \item Use a small learning rate (e.g., 1e-5)
        \item Train for a small number of epochs (e.g, 3 epochs)
        \item Led to SOTA results on many NLU tasks
    \end{itemize}
        \think{How to generate text from BERT?} 
\end{frame}

\begin{frame}
    {Encoder-decoder models}

    An encoder-decoder model encodes input text to a sequence of contextualized representations, and decodes a sequence of tokens autoregressively.
    \begin{align*}
        h_1,\ldots,h_n &= \mathrm{Encoder}(x_1,\ldots,x_n) \\
        s_1,\ldots,s_m &= \mathrm{Decoder}(y_0,\ldots,y_{m-1}, h_1,\ldots,h_n)\\
        p(y_i\mid x, y_{<i}) &= \mathrm{softmax}(Ws_i+b)
    \end{align*}

    \pause
    How do we train the encoder-decoder?
    \begin{itemize}
        \item Use any supervised task, e.g., machine translation 
        \item Use self-supervised learning: predict text spans from their context 
    \end{itemize}
\end{frame}

\begin{frame}
    {Masked language modeling using an encoder-decoder}

    \textbf{Input}: text with corrupted spans\\
    \textbf{Output}: recovered spans

    \begin{figure}
        \includegraphics[height=3cm]{figures/t5-span}
    \end{figure}

    Compare with encoder-only models:\\
    \begin{itemize}
        \item Encoder: predict single tokens based on encoder representation
        \item Encoder-decoder: predict a sequence of tokens (flexibility in objective design)
    \end{itemize}
\end{frame}

\begin{frame}
    {T5: objective}

    \begin{itemize}
        \item First train on unlabele data by \textbf{masked language modeling}
            \begin{itemize}
                \item Predict corrupted spans as a sequence
            \end{itemize}
        \item Then \blue{continue training} by \textbf{supervised multitask learning}
            \begin{itemize}
                \item Formulate tasks as text-to-text format using a prefix to denote the task
                \item Mixing examples from different datasets when constructing batches  
            \end{itemize}
    \begin{figure}
        \includegraphics[height=4cm]{figures/t5-mtl}
    \end{figure}
        \item Jointly training with the two objectives works slightly worse
    \end{itemize}
\end{frame}

\begin{frame}
    {T5: finetune}
    \begin{itemize}
        \item Formulate the task in text-to-text format
        \item Fine-tune all parameters (similar to BERT fine-tuning)
        \item Advantages over encoder models: unified modeling of many different tasks including text generation
    \end{itemize}
\end{frame}

\begin{frame}
    {Decoder-only models}

    A decoder-only model predicts the next token given the prefix autoregressively.
    \begin{align*}
        s_1,\ldots,s_m &= \mathrm{Decoder}(y_0,\ldots,y_{m-1}, h_1,\ldots,h_n)\\
        p(y_i\mid y_{<i}) &= \mathrm{softmax}(Ws_i+b)
    \end{align*}
    (A prefix of $y$ can be the input.)
    \begin{figure}
            \includegraphics[height=0.5\textheight]{figures/decoder}
    \end{figure}
    (more on language models later)
\end{frame}

\begin{frame}
    {Generative Pretraining (GPT)}
    \begin{itemize}
        \item {\bf Model}: 12 layer decoder-only transformer
        \item {\bf Objective}: next word prediction
            $$
            \max \sum_{y\in\sD} \sum_i \log p(y_i\mid y_{<i})
            $$
        \item {\bf Finetuning}: \blue{auxiliary LM objective} $L_{\text{task}} + \lambda L_{\text{LM}}$ (next word prediction on labeled task data)
    \end{itemize}
\end{frame}

\begin{frame}
    {Generative Pretraining (GPT): task-specific finetuning}
    \begin{figure}
        \includegraphics[width=0.9\textwidth]{figures/gpt1}
    \end{figure}
    \begin{itemize}
        \item Single input: linear on top of \texttt{extract}
        \item Multiple input: process each input separately then aggregate
    \end{itemize}
\end{frame}

\begin{frame}
    {Ablation studies of GPT}
    Architecture, pretraining, finetuning: which is critical?
    \begin{figure}
        \includegraphics[width=\textwidth]{figures/gpt-ablation}
    \end{figure}
    \begin{itemize}
        \item Auxiliary objective only helps on larger datasets (MNLI, QQP)
        \item Pretrained transformer $>$ pretrained LSTM (single layer) $>$ non-pretrained transformer
    \end{itemize}
\end{frame}

\begin{frame}
    {Compare with BERT}
    \begin{figure}
        \includegraphics[width=\textwidth]{figures/bert-results}
    \end{figure}
    Medium-sized encoder models tend to work better than decoder-only models when finetuned
\end{frame}

\begin{frame}
    {Encoder-only vs decoder-only models: attention}
    \begin{tikzpicture}
        \node(dec) {\includegraphics[height=2cm]{figures/dec-attn}};
        \node[above=0.5cm of dec] {Decoder-only};
        \node(enc) [right= of dec]{\includegraphics[height=2cm]{figures/enc-attn}};
        \node[above=0.5cm of enc] {Encoder-only};
    \end{tikzpicture}

    \pause
    Encoder-only models provides better embeddings due to bidirectional attention.
\end{frame}

\begin{frame}
    {Encoder-only vs decoder-only models: generation}
    Decoder-only models can make predictions through generation {\em without finetuning}

    \vspace{1em}
    \pause
    \begin{columns}
        \begin{column}{0.4\textwidth}
        \includegraphics[width=\textwidth]{figures/gpt1-zs}
        \end{column}
        \begin{column}{0.6\textwidth}
            Heuristics for zero-shot prediction:
            \begin{itemize}
                \item Sentiment classification: [example] + very + \{positive, negative\} $\quad$ \blue{\em prompting}
                \item Linguistic acceptability: thresholding on log probabilities
                \item Multiple choice: predicting the answer with the highest log probabilities
            \end{itemize}
            \textbf{Scaling trend}: zero-shot performance increases during pretraining
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    {Encoder-only vs decoder-only models: training efficiency}

    On each sequence: 
    \begin{itemize}
        \item Encoder-only models are trained on \blue{15\%} (mask rate) of the tokens
        \item Decoder-only models are trained on \blue{all} tokens 
    \end{itemize}

    \pause
    What about encoder-decoder models?
    \begin{itemize}
        \item Flexibility on encoder design
        \item Limited advantage on long-form generation tasks over decoder-only model 
        \item More resource available for decoder-only models 
    \end{itemize}
\end{frame}

% TODO: more on data curation (Olmo, LLM360?)?
\begin{frame}
    {What are these models trained on?}

    Both quantity and quality are important
    \begin{itemize}
        \item Wikipedia: encyclopedia articles (\green{clean}, \red{single domain})
        \item Toronto Books Corpus: e-books (\green{diverse domain}) 
        \item WebText (40GB): content submitted to Reddit with a vote $\ge 3$ (\green{diverse}, \red{bias}) 
        \item CommonCrawl (20TB): scraped HTML with markers removed (\green{diverse, large}, \red{noisy, bias})
            \begin{itemize}
                \item A cleaned version: C4 (750GB)
            \end{itemize}
    \end{itemize}

    Active research area: What data is good for pretraining? 
\end{frame}

\section{Efficient pretraining}

\begin{frame}
    {Overview}
    Approaches to speed up pretraining \pause
    \begin{itemize}
        \itemsep1em
        \item Reduce model size
        \item Design more sample-efficient learning objectives
        \item Improve efficiency of self-attention
        \item Improve system-level efficiency
    \end{itemize}
\end{frame}

\begin{frame}
    {Approach 1: Reduce model size}

    Idea 1: reduce the number of parameters

    ALBERT (a lite BERT) \mycite[https://arxiv.org/abs/1909.11942]{[Lan et al., 2020]}

    \begin{itemize}
            \only<1>{
        \item \textbf{Factorization}:
    \begin{itemize}
        \item Recall that in Transformer, we first need to map the one-hot encoding (of size $\red{V}$) of a token to Q, K, V embeddings (of size $\blue{H}$)
        \item The number of parameters is $\red{V}\times \blue{H}$
        \item We can instead first map it to a lower-dim space (of size $\green{E}$) so that the number of params is $\red{V}\times\green{E} + \green{E}\times\blue{H}$
    \end{itemize}
}
            \only<2->{
\item \textbf{Parameter sharing}:
    \begin{itemize}
        \item Share feedforward network weights across layers
        \item Share self-attention weights across layers
        \item ALBERT: share all params across layers
    \end{itemize}
}
    \end{itemize}
\end{frame}

\begin{frame}
    {Approach 1: Reduce model size}

    Idea 2: reduce interaction among parameters (sparse/modular architectures)

    DEMix \mycite[https://aclanthology.org/2022.naacl-main.407.pdf]{[Gururangan et al., 2022]}

    \begin{columns}
        \begin{column}{0.4\textwidth}
            \includegraphics[height=0.6\textheight]{figures/demix}
        \end{column}
        \begin{column}{0.6\textwidth}
            \begin{itemize}
                \item Replace the FFN layer with an ensemble of $n$ experts
                \item Route examples to experts corresponding to its domain determinstically
                    $$
                    \mathrm{FFN}(h) = \sum_{i=1}^n \1{[x \in \text{domain $i$}]} \mathrm{FFN}_i(x)
                    $$
                \item Only a subset of params are active for each example/batch
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    {Approach 1: Reduce model size}

    Idea 2: reduce interaction among parameters (sparse/modular architectures)

    Branch-Train-Merge \mycite[https://arxiv.org/pdf/2208.03306.pdf]{[Li et al., 2022]}
    \begin{figure}
            \includegraphics[width=\textwidth]{figures/btm}
    \end{figure}
    \begin{itemize}
        \item Train domain experts in parallel and ensemble them (or take weighted average of their parameters)
        \item Reduce synchronization among GPUs at the cost of increased model size
        \item Easy to expand/remove domain experts 
    \end{itemize}
\end{frame}

\begin{frame}
    {Approach 2: design sample-efficient learning objectives}

    ALBERT: Inter-sentence coherence loss\\
    \begin{itemize}
        \item Motivation: the next sentence prediction task is too easy
        \item Design \blue{hard negative examples}
        \item Input: take two consecutive sentences, swap their order randomly
        \item Output: predict if they are in natural order\\
            \begin{tabular}{ll}
                \textit{I went home. \texttt{SEP} I slept.} & +1\\
                \textit{I slept. \texttt{SEP} I went home.} & -1\\
            \end{tabular}
            
        \item Model needs to learn temporal order of events (commonsense, causality etc.) 
    \end{itemize}
\end{frame}

\begin{frame}
    {Approach 2: design sample-efficient learning objectives}

    ELECTRA \mycite[https://arxiv.org/abs/2003.10555]{[Clark et al., 2020]}: discriminate from true vs guessed tokens

    \vspace{-1em}
    \begin{figure}
        \includegraphics[height=3cm]{figures/electra}
    \end{figure}
    \vspace{-1em}

    \begin{itemize}
        \item First train the generator for n steps using the MLM objective.
        \item Freeze generator weights. Train the discriminator using the sequence classification objective. Keep discriminator for finetuning.
        \item Comparison with MLM: predict at every position; hard negative examples. 
    \end{itemize}
\end{frame}

\begin{frame}
    {Approach 2: design sample-efficient learning objectives}

    ELECTRA result:
    \begin{figure}
        \includegraphics[height=3cm]{figures/electra-result}
        \caption{Finetuning result on the GLUE benchmark}
    \end{figure}
    \begin{itemize}
        \item Larger improvement at smaller model sizes
        \item Faster training 
        \item An effective approach if you don't have large compute for pretraining
    \end{itemize}
\end{frame}

\begin{frame}
    {Approach 3: alternatives to self-attention}

    Transformer recap
    \begin{columns}
        \begin{column}{0.4\textwidth}
    \begin{figure}
        \includegraphics[width=\textwidth]{figures/transformer-block}
        \caption{From \href{https://jalammar.github.io/illustrated-transformer}{The Illustrated Transformer}}
    \end{figure}
        \end{column}
        \begin{column}{0.6\textwidth}
            Which components require matrix multiplication?\\
            \pause
            \begin{itemize}
                \item Self-attention 
                    \begin{itemize}
                        \item Q,K,V projection
                        \item Scaled dot-product attention
                    \end{itemize}
                \item Feed-forward layer
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    {Compute cost of transformers}
    Q, K, V projection:\\[1ex]
    \begin{tikzpicture}
        \draw (0,0) rectangle (2, 1) node[pos=.5] {$n\times d_e$};
        \draw (4, 0) rectangle (4+3, 1) node[pos=.5] {$n\times d$};
        \draw[arrow] (2.5, 0.5) -- (3.5, 0.5) node[midway, above] {linear};
        \onslide<2->{
            \node at(4+3+2, 0.5) {$O(n\times d_e \times d)$};
        }
    \end{tikzpicture}

    \medskip\onslide<+->{
    Scaled dot-product attention:\\[1ex]
    \begin{tikzpicture}
        \draw (0,0) rectangle (3, 1) node[pos=.5] {$n\times d$};
        \draw (4, 0) rectangle (4+1, 3) node[pos=.5] {$d\times n$};
        \draw (4+1+2, 0) rectangle (4+1+2+1, 1) node[pos=.5] {$n\times n$};
        \draw[arrow] (4+1+0.5, 0.5) -- (4+1+2-0.5, 0.5) node[midway, above] {matmul};
        \onslide<+->{
            \node at(4+1+2+1+2, 0.5) {$O(d\times n^2)$};}
    \end{tikzpicture}
    }
\end{frame}

\begin{frame}
    {Compute cost of transformers}

    Feed-forward layer (GPT-2):\\[1ex]
    \begin{tikzpicture}
        \draw (0,0) rectangle (2, 1) node[pos=.5] {$n\times d$};
        \draw (4+1, 0) rectangle (4+3+1, 1) node[pos=.5] {$n\times d_h$};
        \draw[arrow] (2.5, 0.5) -- (4+1-0.5, 0.5) node[midway, above] {linear+ReLU};
        \onslide<2->{
        \draw (4+3+1+3, 0) rectangle (4+3+1+3+2, 1) node[pos=.5] {$n\times d$};
        \draw[arrow] (4+3+1+0.5, 0.5) -- (4+3+1+3-0.5, 0.5) node[midway, above] {linear+ReLU};}
    \onslide<.->{
        \node at(0, -1) {$O(n\times d\times d_h)$};
    }
    \end{tikzpicture}

    \onslide<+->{
    \begin{itemize}
        \item Two-layer FFN
        \item $d_h=4d$ ($d>1K$) by default in GPT-2
        \item Approximately half of the compute time
    \end{itemize}
    }
\end{frame}

\begin{frame}
    {Improve efficiency of self-attention (for long sequences)}

    \onslide<2->{
    \textbf{Key idea}: reduce the $O(n^2)$ time and memory cost\\
    \begin{itemize}
        \item Sparsify the attention matrix
            % TODO: draw a figure to show the masking
            \begin{itemize}
                \item Deterministic mask
                \item Data-dependent mask (Reformer \mycite[https://arxiv.org/pdf/2001.04451.pdf]{[Kitaev et al., 2020]})
            \end{itemize}
        \item Compress the key-value memory
            % TODO: draw a figure to show the size change 
            \begin{itemize}
                \item Low-rank projection 
                \item Attention-based projection 
            \end{itemize}
    \end{itemize}
    }

\end{frame}

\begin{frame}
    {Sparse attention}
    \textbf{Longformer} \mycite[https://arxiv.org/pdf/2004.05150.pdf]{[Beltagy et al., 2020]}: attention within a local window\\[1ex]

        {\includegraphics[width=0.8\textwidth]{figures/longformer}}

    \begin{itemize}
        \item \blue{Sliding window}: attending to a \textit{local} window of size $w$ around each token $\red{O(n\times w)}$
        \item \blue{Dilated sliding window}: reaching \textit{longer range} with a larger window size with gaps
        \item \blue{Global window}: \textit{full attention} on specific tokens, e.g., \texttt{[CLS]} in BERT
            \pause
        \item Details: balancing efficiency and performance
            \begin{itemize}
                \item Adding dilation on some heads
                \item Using small window size on lower layers and larger ones on higher layers
            \end{itemize}
    \end{itemize}
\end{frame}

\begin{frame}
    {Compresse the KV memory}

    Self-attention is low rank \href{https://arxiv.org/pdf/2006.04768.pdf}{[Wang et al., 2020]}\\[1ex]
    {\centering
    \includegraphics[height=3cm]{figures/low-rank}}

    \begin{itemize}[<+->]
        \item Left: cumulative eigenvalues of pretrained transformer with $n=512$
            \begin{itemize}
                \item Most information in the attention matrix can be recovered by the top 128 eigenvectors %explains $>90\%$ of the variance
            \end{itemize}
        \item Right: cumulative eigenvalues of the top 128 eigenvalues across layers 
            \begin{itemize}
                \item Higher layers are more low-rank 
            \end{itemize}
        \item \textbf{Idea}: instead of attending to $n$ tokens, attend to $k$ principal components 
    \end{itemize}
\end{frame}

\begin{frame}
    {Summarize the KV memory}

    \textbf{Linformer} \mycite[https://arxiv.org/pdf/2006.04768.pdf]{[Wang et al., 2020]}: compute self-attention in a lower dimension \\[1em]

    \begin{columns}
        \begin{column}{0.3\textwidth}
        {\includegraphics[width=\textwidth]{figures/linformer}}
        \end{column}
        \begin{column}{0.7\textwidth}
            \begin{itemize}[<+->]
                \item Reduce dimensionality of the ``memory'': Map K, V from $n\times d$ to $\blue{k}\times d$
                \item Attend to the lower-dimensional memory: $\mathrm{softmax}\p{Q_{n\times d}K^T_{k\times d}/\sqrt{d}}$
                    \begin{itemize}
                        \item What's the dimension of the attention matrix?
                            \pdfnote{n x k}
                        \item What's the dimension of the self-attention output?
                            \pdfnote{n x d}
                    \end{itemize}
                \item Computation cost: \red{$O(nk)$} (linear in $n$)
                \item Downside of uisng Linformer as a decoder?
                    \begin{itemize}
                        \item Unclear how to mask: past and future are mixed
                    \end{itemize}
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}
    {Summary on efficient self-attention}

    Improve the quadratic time and space complexity of self-attention\\
    \begin{itemize}
        \item Sparsify the attention matrix
        \item Compress the KV memory
    \end{itemize}

    \pause
    Bad news: Most techniques are not widely used in large pretrained models now. Why?\\
    \begin{itemize}
        \item Improvement in time/space complexity doesn't always translate to real time/space savings
        \item These techniques often breaks structure and sacrifice the batching ability on GPUs
        \item Only see improvement on very long sequences
    \end{itemize}
    %\pause
    %Takeaways:\\
    %\begin{itemize}
    %    \item Attention structure is important
    %    \item Low-rank techniques
    %\end{itemize}
\end{frame}

\begin{frame}
    {Approach 4: system-level approaches}
    \begin{itemize}
        \item Operates at a lower abstraction level
        \item Often brings more direct impact on efficiency
        \item Example:
            \begin{itemize}
                \item Gradient accumulation
                \item Model and data parallelism (e.g., deepspeed)
                \item Flash attention: exploit GPU memory asymmetry
                    \begin{figure}
    \includegraphics[width=0.6\textwidth]{figures/flash-attention}\\
                    \end{figure}
            \end{itemize}
    \end{itemize}
\end{frame}


\end{document}
